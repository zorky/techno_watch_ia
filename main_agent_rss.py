#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Agent RSS avec r√©sum√© automatique via LLM local.

Ce script / cli ex√©cute ces actions, dans l'ordre :
- Lit une liste de flux RSS
- Filtre les articles selon des mots-cl√©s
- R√©sume les articles avec un mod√®le LLM local (Ollama)
- Affiche les r√©sultats en console
- Envoi la revue de veille par mail

Framework : LangGraph pour le graphe des actions (noeuds)

Usage :
    python main_agent_rss.py [--debug]

Installation et Configuration :

 - Installer les d√©pendances requises avec uv ou pip :

   ```
   uv venv
   source .venv/bin/activate # ou source .venv/Scripts/activate sous Windows
   uv sync --dev
   ```
   ou pip
   ```
   python3 -m venv .venv
   source .venv/bin/activate # ou source .venv/Scripts/activate sous Windows
   pip install -r requirements.txt
   ```
   paquets : langgraph, langchain, langchain_core, pydantic, feedparser

 - un fichier .env est possible pour surcharger des variables, voir le .env.example :

   LLM_MODEL (par d√©faut mistal),
   LLM_TEMPERATURE (par d√©faut 0.3),
   OLLAMA_BASE_URL (par d√©faut http://localhost:11434/v1)
   LIMIT_ARTICLES_TO_RESUME (par d√©faut -1 : pas de limite) : pour l'inf√©rence, combien d'articles le LLM doit r√©sumer ?
   RSS_URLS (par d√©faut la liste dans _get_rss_urls) : sur une seule ligne

   Exemple :

 LLM_MODEL=mistral
 # LLM_MODEL=llama3:8b-instruct-q4_K_M
 LLM_TEMPERATURE=0.7
 RSS_URLS=["https://belowthemalt.com/feed/","https://cosmo-games.com/sujet/ia/feed/","https://www.ajeetraina.com/rss/","https://rss.nytimes.com/services/xml/rss/nyt/Technology.xml"]


 - Ollama doit √™tre ex√©cut√© en local avec le mod√®le pull√©, ou tout autre serveur LLM

"""

import logging
from datetime import datetime, timedelta
from colorama import Fore, Style
from dotenv import load_dotenv
from langgraph.graph import StateGraph
from langchain_core.runnables import RunnableLambda

from langchain_openai import ChatOpenAI
# from langchain_ollama import ChatOllama

import feedparser
import os

from sentence_transformers import SentenceTransformer

from models.states import RSSState
from read_opml import parse_opml_to_rss_list

from bs4 import BeautifulSoup

from core import measure_time, argscli

# =========================
# Configuration du mod√®le LLM et configurations recherche
# =========================

load_dotenv()

LLM_MODEL = os.getenv("LLM_MODEL", "mistral")
LLM_API = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434/v1")  # si ChatOpenAI
# LLM_API = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")  # si ChatOllama
LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.3"))
# Mod√®les embeddings disponibles et sp√©cs : https://www.sbert.net/docs/sentence_transformer/pretrained_models.html
MODEL_EMBEDDINGS="all-MiniLM-L6-v2"
# MODEL_EMBEDDINGS="all-mpnet-base-v2"

FILTER_KEYWORDS = os.getenv("FILTER_KEYWORDS", "").split(",")
THRESHOLD_SEMANTIC_SEARCH = float(os.getenv("THRESHOLD_SEMANTIC_SEARCH", "0.5"))
MAX_DAYS = int(os.getenv("MAX_DAYS", "10"))
OPML_FILE = os.getenv("OPML_FILE", "my.opml")
TOP_P = float(os.getenv("TOP_P", "0.5"))
MAX_TOKENS_GENERATE = int(os.getenv("MAX_TOKENS_GENERATE", "300"))

# =========================
# Init du logging et logger
# Par d√©faut INFO
# Forcer √† DEBUG : python main_agent_rss.py --debug
# =========================

logging.basicConfig(level=logging.INFO)
from core import logger

# =========================
# Configuration LLM local / saas
# =========================
def init_llm_chat():
    return ChatOpenAI(
        model=LLM_MODEL,
        openai_api_base=LLM_API,
        openai_api_key="dummy-key-ollama",
        temperature=LLM_TEMPERATURE,
        top_p=TOP_P,
        max_tokens=MAX_TOKENS_GENERATE
    )
    # return ChatOllama(
    #     model=LLM_MODEL,
    #     temperature=LLM_TEMPERATURE,
    #     base_url=LLM_API,  # http://localhost:11434
    #     top_p=TOP_P,
    #     # num_predict=MAX_TOKENS,
    # )

llm = init_llm_chat()

# =========================
# Configuration du mod√®le d'embeddings
# Mod√®les disponibles et sp√©cs : 
# https://www.sbert.net/docs/sentence_transformer/pretrained_models.html
# =========================

def get_device_cpu_gpu_info():
    import torch
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        logger.info(Fore.GREEN + f"GPU disponible : {gpu_name}")
        return "cuda"    
    logger.info(Fore.YELLOW + "Aucun GPU disponible, utilisation du CPU.")    
    return "cpu"

DEVICE_TYPE = get_device_cpu_gpu_info()

def init_sentence_model():
    logger.info(Fore.GREEN + f"Init SentenceTransformer {MODEL_EMBEDDINGS} sur {DEVICE_TYPE}")
    return SentenceTransformer(MODEL_EMBEDDINGS, device=DEVICE_TYPE)
    # return SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device=DEVICE_TYPE)  # bon compromis pour le fran√ßais/anglais
    # return SentenceTransformer('multi-qa-MiniLM-L6-cos-v1', device=DEVICE_TYPE)  # Optimis√© pour la similarit√©

model = init_sentence_model()


# =========================
# Fonctions utilitaires
# =========================

def preprocess_text(text):
    from nltk.stem import WordNetLemmatizer
    from nltk.tokenize import word_tokenize
    from nltk.corpus import stopwords
    import nltk
    import string

    nltk.download('punkt_tab')
    nltk.download('stopwords')
    nltk.download('wordnet')

    # Tokenization
    tokens = word_tokenize(text.lower())
    # Suppression des stopwords et ponctuation
    tokens = [t for t in tokens if t not in stopwords.words('french') and t not in string.punctuation]
    # Lemmatisation
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(t) for t in tokens]
    return " ".join(tokens)

@measure_time
def filter_articles_with_faiss(
    articles,
    keywords: list[str],
    threshold=0.7,
    index_path="keywords_index.faiss",
    show_progress=False,
):
    """
    Filtre les articles par similarit√© s√©mantique avec les mots-cl√©s.
    :param articles: Liste de dicts avec 'title' et 'summary'
    :param keywords: Liste de mots-cl√©s
    :param threshold: Seuil de similarit√© (0 √† 1)
    :return: Articles filtr√©s
    """
    import faiss

    logger.info(f"Filtrage s√©mantique avec les mots-cl√©s {keywords}")
    logger.info(f"Filtrage s√©mantique avec seuil {threshold}...")

    @measure_time
    def get_or_create_index(keywords, model, index_path):
        if os.path.exists(index_path):
            logger.info("üîç Chargement de l'index FAISS existant...")
            return faiss.read_index(index_path)
        else:
            logger.info("üîß Cr√©ation d'un nouvel index FAISS...")
            keyword_embeddings = model.encode(
                keywords, convert_to_tensor=True, show_progress_bar=show_progress
            )
            keyword_embeddings = keyword_embeddings.cpu().numpy()                        
            faiss.normalize_L2(keyword_embeddings)
            index = faiss.IndexFlatIP(keyword_embeddings.shape[1]) # Produit scalaire √©quivalent similarit√© cos
            index.add(keyword_embeddings)

            faiss.write_index(index, index_path)
            return index

    # Cr√©er un index FAISS pour le produit scalaire (similarit√© cosinus)
    index = get_or_create_index(keywords, model, index_path)

    filtered = []
    for article in articles:
        text = f"{article['title']} {article['summary']}".strip()
        if not text:
            continue  # Sauter les articles sans contenu
        # cleaned_text = preprocess_text(text)
        
        article_embedding = model.encode(
            [text], convert_to_tensor=True, show_progress_bar=False
        )
        article_embedding = article_embedding.cpu().numpy()
        faiss.normalize_L2(article_embedding)  # Normaliser l'embedding de l'article

        # Recherche
        similarities, indices = index.search(article_embedding, k=len(keywords)) # k = top N mot-cl√© le plus proche
        max_similarity = similarities[0].max()  # La similarit√© est d√©j√† entre 0 et 1

        if max_similarity >= threshold:
            matched_keywords = [
                keywords[i] for i in indices[0] if similarities[0][i] >= threshold
            ]
            # logger.info(
            #     f"Similarities: {similarities[0]}, Indices: {indices[0]}"
            # )
            logger.info(
                f"‚úÖ Article retenu (sim={max_similarity:.2f}, mots-cl√©s: {matched_keywords}): {article['title']} {article['link']}"
            )
            # article['scoring'] = round(max_similarity, 2)
            article['scoring'] = f"{max_similarity * 100:.1f} %"
            logger.info(
                f"{article['title']} -> {article['scoring']}"
            )
            filtered.append(article)

    logger.info(
        f"üìä {len(filtered)}/{len(articles)} articles apr√®s filtrage s√©mantique (seuil={threshold})"
    )
    return filtered


def set_prompt(theme, title, content):    
    prompt = f"""Tu es un expert en {theme}. R√©sume **uniquement** l'article ci-dessous en **3 phrases maximales**, en fran√ßais, avec :
1. L'information principale (qui ? quoi ?), pr√©cise s'il y a du code ou un projet avec du code.
2. Les d√©tails cl√©s (chiffres, noms, dates).
3. L'impact ou la solution propos√©e.

**Exemple :**
Titre : "Sortie de Python 3.12 avec un compilateur JIT"
Contenu : "Python 3.12 int√®gre un compilateur JIT exp√©rimental..."
R√©sum√© : Python 3.12 introduit un compilateur JIT exp√©rimental pour acc√©l√©rer l'ex√©cution. Les tests montrent un gain de 10 √† 30% sur certains workloads. Disponible en version b√™ta d√®s septembre 2025.

**√Ä r√©sumer :**
{title}

Contenu : {content}

R√©sum√© :"""

    return prompt

    # minimaliste et original
    #     prompt = f"""Tu es un journaliste expert. R√©sume en fran√ßais cet article en 3 phrases claires et concises.
    # Titre : {title}
    # Contenu : {content}
    # """
    # prompt technique √† points
    #     prompt = f"""Tu es un expert en {theme}. R√©sume cet article en 3 phrases **techniquement pr√©cises**, en fran√ßais, en extraant :
    # 1. L'information principale (ex: une d√©couverte, une vuln√©rabilit√©, une sortie logicielle).
    # 2. Les d√©tails cl√©s (ex: versions concern√©es, acteurs impliqu√©s, dates).
    # 3. L'impact ou la nouveaut√© (ex: "Cette faille affecte X utilisateurs", "Ce framework simplifie Y").
    # - Si le contenu est trop vague, r√©ponds : "R√©sum√© impossible : article incomplet ou non informatif.
    # - Si le contenu est en anglais, traduis-le d'abord en fran√ßais avant de r√©sumer.

    # **Titre :** {title}
    # **Contenu :** {content}

    # **R√©sum√© :**"""
    # few-shot
    #     prompt = f"""Exemples de r√©sum√©s attendus :
    # ---
    # Titre : "D√©couverte d'une faille critique dans OpenSSL 3.2"
    # Contenu : "La faille CVE-2024-1234 permet une ex√©cution de code √† distance..."
    # R√©sum√© : OpenSSL 3.2 contient une faille critique (CVE-2024-1234) permettant une ex√©cution de code √† distance. Les versions 3.2.0 √† 3.2.3 sont concern√©es. Les utilisateurs doivent mettre √† jour imm√©diatement.
    # ---

    # Titre : "Meta pr√©sente Llama 3.1 avec 400M de param√®tres"
    # Contenu : "Llama 3.1 introduit une architecture optimis√©e pour les devices mobiles..."
    # R√©sum√© : Meta a lanc√© Llama 3.1, un mod√®le l√©ger (400M de param√®tres) optimis√© pour les mobiles. Il surpasse les pr√©c√©dents mod√®les sur les benchmarks de latence. Disponible d√®s aujourd'hui en open source.
    # ---

    # **√Ä toi :** R√©sume l'article suivant en suivant le m√™me format.

    # **Titre :** {title}
    # **Contenu :** {content}

    # **R√©sum√© :**"""

@measure_time
def summarize_article(title, content):    
    prompt = set_prompt("IA, ing√©nieurie logicielle et cybers√©curit√©", title, content)

    if argscli.debug:
        logger.debug(
            Fore.MAGENTA
            + "--- PROMPT ENVOY√â AU LLM ---\n"
            + prompt
            + "\n---------------------------"
        )
    # Appel au LLM
    result = llm.invoke(prompt)

    if argscli.debug:
        logger.debug(
            Fore.MAGENTA
            + "--- R√âPONSE BRUTE DU LLM ---\n"
            + str(result)
            + "\n---------------------------"
        )
    summary = result.content.strip().strip('"').strip()
    # Nettoyage des introductions g√©n√©riques
    for prefix in ["Voici un r√©sum√© :", "R√©sum√© :", "L'article explique que"]:
        if summary.startswith(prefix):
            summary = summary[len(prefix) :].strip()
    return summary


def strip_html(text: str) -> str:
    """Supprime les balises HTML d'un texte pour n'avoir que du texte brut."""
    return BeautifulSoup(text, "html.parser").get_text()


def get_summary(entry: dict):
    """
    Affiche le r√©sum√© ou le contenu d'une entr√©e de flux RSS ou Atom
       RSS 2.0: 'summary'
       Atom: 'content'

    Args:
        entry: Une entr√©e de flux RSS/Atom.
    """
    if "content" in entry.keys():
        content: list[feedparser.FeedParserDict] = entry.get("content", [dict])
        return content[0].get("value", "Pas de r√©sum√©")
    
    return entry.get("summary", "Pas de r√©sum√©")


def add_article_with_entry_syndication(entry, articles, cutoff_date, recent_in_feed):
    """
    A partir du contenu d'une entr√©e entry, ajoute un article r√©cent √† la liste des articles √† traiter.

    Args:
        entry: Un article du flux RSS/Atom.
        articles: La liste des articles √† traiter.
        cutoff_date: La date limite pour qu'un article soit consid√©r√© comme r√©cent.
        recent_in_feed: Le compteur d'articles r√©cents dans le flux actuel.
    """
    # R√©cup√©ration de la date de publication (priorit√© √† published, sinon updated)
    published_time = None
    if hasattr(entry, "published_parsed"):
        published_time = datetime(*entry.published_parsed[:6])
    elif hasattr(entry, "updated_parsed"):
        published_time = datetime(*entry.updated_parsed[:6])

    # V√©rification de la date
    is_recent = published_time and (published_time >= cutoff_date)
    logger.debug(
        f"Article: {getattr(entry, 'title', 'Sans titre')} "
        f"(publi√© le {published_time}) : {'r√©cent' if is_recent else 'trop ancien' if published_time else 'date inconnue'}"
    )

    if is_recent:
        # Normalisation des champs (RSS/Atom)
        title = getattr(entry, "title", "Sans titre")
        summary = get_summary(entry)
        summary = strip_html(summary)  # Nettoyage du HTML
        logger.debug(f"R√©sum√© brut (apr√®s nettoyage) : {summary}")
        link = getattr(entry, "link", "#")
        if isinstance(link, list):  # Cas Atom o√π link est un objet
            link = link[0].href if link else "#"
        logger.info(Fore.GREEN + f"üÜï Article r√©cent : {title} ({link})")
        articles.append(
            {
                "title": title,
                "summary": summary,
                "link": link,
                "published": published_time.isoformat() if published_time else None,
                "scoring": "0 %"
            }
        )
        recent_in_feed += 1
    return recent_in_feed


@measure_time
def fetch_rss_articles(rss_urls: list[str], max_age_days: int = 10):
    """
    R√©cup√®re les articles des flux RSS/Atom, en ne gardant que ceux publi√©s dans les `max_age_days` derniers jours.

    Args:
        rss_urls: Liste des URL des flux RSS/Atom.
        max_age_days: Nombre de jours pour consid√©rer un article comme r√©cent.
    """
    AGENT = "ReaderRSS/1.0"
    RESOLVE_RELATIVE_URIS = False
    SANITIZE_HTML = True
    articles = []
    cutoff_date = datetime.now() - timedelta(days=max_age_days)
    logger.info(f"seuil date : {cutoff_date}")

    for url in rss_urls:
        logger.info(Fore.BLUE + f"Lecture du flux RSS : {url}")
        feed = feedparser.parse(
            url,
            resolve_relative_uris=RESOLVE_RELATIVE_URIS,
            sanitize_html=SANITIZE_HTML,
            agent=AGENT,
        )  # voir Etag et modified pour ne pas tout recharger
        recent_in_feed = 0

        for entry in feed.entries:
            recent_in_feed = add_article_with_entry_syndication(
                entry, articles, cutoff_date, recent_in_feed
            )

        logger.info(
            f"{len(feed.entries)} articles trouv√©s dans ce flux, {recent_in_feed} r√©cents !"
        )

    logger.debug(f"{len(articles)} articles r√©cents r√©cup√©r√©s")
    return articles

# =========================
# N≈ìuds du graphe
# =========================
def fetch_node(state: RSSState) -> RSSState:
    logger.info("üì• R√©cup√©ration des articles...")

    articles = fetch_rss_articles(state.rss_urls, MAX_DAYS)
    logger.info(f"{len(articles)} articles r√©cup√©r√©s")
    logger.info(f"{articles[0]['title']} {articles[0]['scoring']}")
     
    return state.model_copy(update={"articles": articles})


def filter_node(state: RSSState) -> RSSState:
    logger.info("üîç Filtrage des articles par mots-cl√©s...")

    filtered = filter_articles_with_faiss(
        state.articles, state.keywords, threshold=THRESHOLD_SEMANTIC_SEARCH
    )
    logger.info(f"{len(filtered)} articles correspondent aux mots-cl√©s (s√©mantique)")

    # filtered = filter_articles_with_tfidf(state.articles, state.keywords, threshold=0.3)
    # logger.info(f"{len(filtered)} articles correspondent aux mots-cl√©s (s√©mantique)")

    return state.model_copy(update={"filtered_articles": filtered})


def summarize_node(state: RSSState) -> RSSState:
    logger.info("‚úèÔ∏è  R√©sum√© des articles filtr√©s...")
    LIMIT_ARTICLES_TO_RESUME = int(os.getenv("LIMIT_ARTICLES_TO_RESUME", -1))
    if LIMIT_ARTICLES_TO_RESUME > 0:
        logger.info(f"Limite de r√©sum√© √† {LIMIT_ARTICLES_TO_RESUME} articles")
        articles = state.filtered_articles[:LIMIT_ARTICLES_TO_RESUME]
    else:
        logger.info("Pas de limite sur le nombre d'articles √† r√©sumer")
        articles = state.filtered_articles
    summaries = []
    for i, article in enumerate(articles, start=1):
        logger.info(Fore.YELLOW + f"R√©sum√© {i}/{len(articles)} : {article['title']}")
        summary_text = summarize_article(article["title"], article["summary"])
        summaries.append(
            {
                "title": article["title"],
                "summary": summary_text,
                "link": article["link"],
                "scoring": article["scoring"],
                "published": article["published"]
            }
        )
    return state.model_copy(update={"summaries": summaries})


def output_node(state: RSSState) -> RSSState:    
    logger.info("üìÑ Affichage des r√©sultats finaux")
    for item in state.summaries:
        print(
            Fore.CYAN
            + f"\nüì∞ {item['title']}\n"
            + Fore.CYAN
            + f"\nüìà {item['scoring']}\n"
            + Fore.GREEN
            + f"üìù {item['summary']}\n"
            + Fore.BLUE
            + f"üîó {item['link']}\n"
            + f"‚è±Ô∏è {item['published']}"
        )
    return state

def send_articles(state: RSSState) -> RSSState:
    from send_articles_email import send_watch_articles
    from models.emails import EmailTemplateParams    
    logger.info("Envoi mail des articles")
    logger.info(f"Envoi de {len(state.summaries)} articles")
    if len(state.summaries) > 0:
        _params_mail = EmailTemplateParams(
            articles=state.summaries,
            keywords=state.keywords,
            threshold=THRESHOLD_SEMANTIC_SEARCH
        )
        send_watch_articles(_params_mail)    
    return state

def save_articles(state: RSSState) -> RSSState:
    from core import save_to_db
    logger.info("Sauvegarde des articles r√©sum√©s en DB")    
    if len(state.summaries) > 0:
        save_to_db(state.summaries)
    return state

# =========================
# Construction du graphe : noeuds (nodes) et transitions (edges)
# fetch -> filter -> summarize -> output
# =========================
def make_graph():
    graph = StateGraph(RSSState)
    graph.add_node("fetch", RunnableLambda(fetch_node))
    graph.add_node("filter", RunnableLambda(filter_node))
    graph.add_node("summarize", RunnableLambda(summarize_node))
    graph.add_node("displayoutput", RunnableLambda(output_node))
    graph.add_node("savedbsummaries", RunnableLambda(save_articles))
    graph.add_node("sendsummaries", RunnableLambda(send_articles))        

    graph.set_entry_point("fetch")
    graph.add_edge("fetch", "filter")
    graph.add_edge("filter", "summarize")
    graph.add_edge("summarize", "displayoutput")    
    graph.add_edge("displayoutput", "savedbsummaries")
    graph.add_edge("savedbsummaries", "sendsummaries")

    return graph.compile()


def get_rss_urls():
    """
    Obtient la liste des URL RSS √† traiter √† partir des variables d'environnement.
    Le .env ne contient que des types string et au format JSON
    """
    rss_list_opml = parse_opml_to_rss_list(OPML_FILE)
    return [feed.lien_rss for feed in rss_list_opml]

def _show_graph(graph):
    """Affichage du graphe / automate LangGraph qui est utilis√©"""
    def _get_graph(_graph):
        return _graph.get_graph()
    
    def _display_graph_matplot(_graph):
        import matplotlib.pyplot as plt
        import matplotlib.image as mpimg
        import io
        
        # plt.ion()
        png_data = _get_graph(_graph).draw_mermaid_png()
        img = mpimg.imread(io.BytesIO(png_data), format='PNG')
        plt.imshow(img)
        plt.axis('off')
        plt.show()
        plt.pause(0.001)

    def _display_graph_ascii(_graph):
        print(_get_graph(_graph).draw_ascii())

    def _display_device(_graph):
        from IPython.display import Image, display
        display(Image(_get_graph(_graph).draw_mermaid_png()))    

    try:
        # _display_graph_matplot(graph)
        _display_graph_ascii(graph)
    except Exception as e:
        logger.error(f"{e}")        
        

# =========================
# Main
# =========================
def main():
    from core.db import init_db
    logger.info(Fore.MAGENTA + Style.BRIGHT + "=== Agent RSS avec r√©sum√©s LLM ===")
    logger.info(
        Fore.YELLOW
        + Style.BRIGHT
        + f"sur {LLM_API} avec {LLM_MODEL} sur une T¬∞ {LLM_TEMPERATURE} sur les {MAX_DAYS} derniers jours"
    )
    logger.info(Fore.YELLOW + f"Initialisation DB")
    init_db()    
    agent = make_graph()
    if argscli.debug:
        logger.info(f"ü§ñ LangGraph d√©roulera cet automate...")
        _show_graph(agent)
    rss_urls = get_rss_urls()        
    state = RSSState(
        rss_urls=rss_urls,
        keywords=FILTER_KEYWORDS
        if FILTER_KEYWORDS != [""]
        else ["intelligence artificielle", "IA", "cybers√©curit√©", "alerte s√©curit√©"],
    )
    agent.invoke(state)


if __name__ == "__main__":
    main()
