import feedparser
from datetime import datetime, timedelta
import logging

from services.base_fetcher import BaseFetcher
from services.models import Source, SourceType

logging.basicConfig(level=logging.INFO)
from core.logger import logger, Fore
from core import measure_time

class RSSFetcher(BaseFetcher):
    def get_summary(self, entry: dict):
        """
        Affiche le r√©sum√© ou le contenu d'une entr√©e de flux RSS ou Atom
        RSS 2.0: 'summary'
        Atom: 'content'

        Args:
            entry: Une entr√©e de flux RSS/Atom.
        """
        if "content" in entry.keys():
            content: list[feedparser.FeedParserDict] = entry.get("content", [dict])
            return content[0].get("value", "Pas de r√©sum√©")
        
        return entry.get("summary", "Pas de r√©sum√©")

    def strip_html(self, text: str) -> str:
        """Supprime les balises HTML d'un texte pour n'avoir que du texte brut."""
        from bs4 import BeautifulSoup
        return BeautifulSoup(text, "html.parser").get_text()
    
    def add_article_with_entry_syndication(self, entry, articles, cutoff_date, recent_in_feed):
        """
        A partir du contenu d'une entr√©e entry, ajoute un article r√©cent √† la liste des articles √† traiter.

        Args:
            entry: Un article du flux RSS/Atom.
            articles: La liste des articles √† traiter.
            cutoff_date: La date limite pour qu'un article soit consid√©r√© comme r√©cent.
            recent_in_feed: Le compteur d'articles r√©cents dans le flux actuel.
        """
        # R√©cup√©ration de la date de publication (priorit√© √† published, sinon updated)
        published_time = None
        logger.info(Fore.RED + f"DATE published_parsed {entry.published_parsed} -> {entry.published_parsed[:6]}")
        if hasattr(entry, "published_parsed"):
            published_time = datetime(*entry.published_parsed[:6])
        elif hasattr(entry, "updated_parsed"):
            published_time = datetime(*entry.updated_parsed[:6])

        # V√©rification de la date
        is_recent = published_time and (published_time >= cutoff_date)
        logger.debug(
            f"Article: {getattr(entry, 'title', 'Sans titre')} "
            f"(publi√© le {published_time}) : {'r√©cent' if is_recent else 'trop ancien' if published_time else 'date inconnue'}"
        )

        if is_recent:
            # Normalisation des champs (RSS/Atom)
            title = getattr(entry, "title", "Sans titre")
            summary = self.get_summary(entry)
            summary = self.strip_html(summary)  # Nettoyage du HTML
            logger.debug(f"R√©sum√© brut (apr√®s nettoyage) : {summary}")
            link = getattr(entry, "link", "#")
            if isinstance(link, list):  # Cas Atom o√π link est un objet
                link = link[0].href if link else "#"
            logger.info(Fore.GREEN + f"üÜï Article r√©cent : {title} ({link})")
            articles.append(
                {
                    "title": title,
                    "summary": summary,
                    "link": link,
                    "published": published_time.isoformat() if published_time else None,
                    "score": "0 %",
                    "source": SourceType.RSS,
                }
            )
            recent_in_feed += 1
        return recent_in_feed


    @measure_time
    def fetch_articles(self, source: Source, max_days: int) -> list[dict]:
        """Votre logique RSS existante"""
        AGENT = "ReaderRSS/1.0"
        RESOLVE_RELATIVE_URIS = False
        SANITIZE_HTML = True
        articles = []
        cutoff_date = datetime.now() - timedelta(days=max_days)
        logger.info(Fore.BLUE + f"Fetch posts RSS du flux {source.url} depuis la date : depuis {max_days} jours -> {cutoff_date}")
        
        feed = feedparser.parse(
            source.url,
            resolve_relative_uris=RESOLVE_RELATIVE_URIS,
            sanitize_html=SANITIZE_HTML,
            agent=AGENT,
        )  # voir Etag et modified pour ne pas tout recharger

        recent_in_feed = 0

        for entry in feed.entries:
            recent_in_feed = self.add_article_with_entry_syndication(
                entry, articles, cutoff_date, recent_in_feed
            )

        logger.info(
            f"{len(feed.entries)} articles trouv√©s dans ce flux, {recent_in_feed} r√©cents !"
        )

        logger.debug(Fore.CYAN + f"{len(articles)} articles r√©cents r√©cup√©r√©s")
        return articles
    